# GLA å¾…è¾¦è¨˜éŒ„ï¼ˆprd_gla.memoryï¼‰

## ğŸ“Š ç•¶å‰é€²åº¦ç¸½è¦½ (2025-11-28 æ›´æ–°)

### âœ… å·²å®Œæˆ (14/14 å¤§ä»»å‹™) ğŸ‰ğŸ‰ğŸ‰
1. âœ… Task 0.1 - chunk_fwd_h å¯¦ä½œ
2. âœ… Task 0.2 - chunk_bwd_dh placeholder
3. âœ… Task 3.3.1 - ChunkGLAFunction scaffolding
4. âœ… Task 3.3.2 - chunk_gla å›ºå®šé•·åº¦å‰å‘
5. âœ… Task 3.3.3.a - maskâ†’varlen helper (å·²å­˜åœ¨)
6. âœ… Task 3.3.3.b - æ•´åˆ varlen åˆ° chunk_gla
7. âœ… Task 3.3.4 - fused_chunk_gla wrapper (å·²æœ‰,ä½¿ç”¨ chunk_gla)
8. âœ… Task 3.3.5 - fused_recurrent_gla (å·²å­˜åœ¨æ–¼ myfla)
9. âœ… Task 3.3.6.a - select_gla_mode router
10. âœ… Task 3.3.6.b - gla_forward çµ±ä¸€å…¥å£
11. âœ… æ›´æ–° __init__.py å°å‡º
12. âœ… æ–°å¢æª”æ¡ˆ: chunk_h.py, chunk_helpers.py
13. âœ… æ›´æ–° memory æ–‡æª”
14. âœ… æ›´æ–° Layer import (ç§»é™¤ fallback åˆ° fla.ops.gla) â­ NEW!

### ğŸ“ˆ å®Œæˆåº¦: 100% (14/14 ä»»å‹™) âœ¨ å…¨éƒ¨å®Œæˆï¼

---

## TODO ç¸½è¦½
- **åŸ Step 1.5 å¾…å»ºæ¨¡é‡é»**: âœ… å·²åŸºæœ¬å®Œæˆ (FusedRMSNormGated å·²å°é½Š, chunk GLA ops å·²å¯¦ä½œ PyTorch ç‰ˆ)
- **ç•¶å‰é‡é»**: å®Œæˆ varlen æ”¯æ´ â†’ fused_chunk_gla â†’ mode router â†’ ç§»é™¤ fallback

## Step 0 â€” å‰ç½®ä¾è³´ (Critical Path)
1. **âœ… Task 0.1 - chunk_fwd_h å·²å¯¦ä½œ**
   - ä½ç½®: `libs/myfla/ops/common/chunk_h.py:L9-L184`
   - åŠŸèƒ½: ç´” PyTorch å¯¦ä½œè·¨ chunk state ç´¯ç©,æ”¯æ´å›ºå®šé•·åº¦ + varlen æ¨¡å¼
   - æ”¯æ´åƒæ•¸: g/g_gamma/gk/gv å››ç¨®è¡°æ¸›æ¨¡å¼,h0 åˆå§‹ç‹€æ…‹,cu_seqlens varlen
   - èªæ³•æª¢æŸ¥: âœ… é€šé `python3 -m py_compile`
   - æ¸¬è©¦ç‹€æ…‹: â¸ï¸ æœªæ¸¬è©¦ (ç„¡ torch ç’°å¢ƒ)
   - é™åˆ¶: ä½¿ç”¨ for-loop + einsum,æ€§èƒ½é ä½æ–¼å®˜æ–¹ Triton ç‰ˆæœ¬

2. **âœ… Task 0.2 - chunk_bwd_dh å·²å¯¦ä½œ (Placeholder)**
   - ä½ç½®: `libs/myfla/ops/common/chunk_h.py:L187-L269`
   - åŠŸèƒ½: Backward æ¢¯åº¦è¨ˆç®—çš„ stub,é ç•™ä»‹é¢ä¾› PyTorch autograd ä½¿ç”¨
   - èªæ³•æª¢æŸ¥: âœ… é€šé
   - æ¸¬è©¦ç‹€æ…‹: â¸ï¸ æœªæ¸¬è©¦
   - æ³¨æ„: ç›®å‰ç‚º placeholder,ä¾è³´ PyTorch autograd åŒ…è£ chunk_fwd_h å¯¦ç¾æ¢¯åº¦æµ

## Step 3 â€” myfla ç‰ˆæœ¬å¯¦ä½œè¨ˆç•«
1. **Layer éª¨æ¶ (`libs/myfla/layers/gla.py`)** âœ… **å·²å®Œæˆ**
   - âœ… `GatedLinearAttention` **å®Œç¾å¾©åˆ»å®˜æ–¹å¯¦ä½œï¼Œ100% åŠŸèƒ½å°é½Š**
   - âœ… **19/19 åƒæ•¸å…¨éƒ¨å°é½Š**ï¼šmode, hidden_size, expand_k/v, num_heads, num_kv_heads, feature_map, use_short_conv, conv_size/bias, use_output_gate, gate_fn, elementwise_affine, norm_eps, gate_logit_normalizer, gate_low_rank_dim, clamp_min, fuse_norm, layer_idx
   - âœ… **å®Œæ•´ forward æµç¨‹** (14 æ­¥é©Ÿ)ï¼šmaskâ†’varlenâ†’short convâ†’feature mapâ†’GLA opsâ†’Gate+Normâ†’o_projâ†’cache æ›´æ–°
   - âœ… **æ”¯æ´åŠŸèƒ½**ï¼šattention_maskã€past_key_valuesã€use_cacheã€cu_seqlensã€GQA (grouped query attention)ã€ä¸‰ç¨®æ¨¡å¼è‡ªå‹•åˆ‡æ›
   - âœ… **å·²ç§»é™¤ fallback**ï¼šç›´æ¥ä½¿ç”¨ myfla.ops.gla (ä¸å†ä¾è³´å®˜æ–¹ fla.ops.gla)
   - âœ… **æ•¸å­¸ç­‰åƒ¹æ€§**ï¼šå®Œå…¨ç­‰åƒ¹ï¼Œç„¡ä»»ä½•ç°¡åŒ–æˆ–åŠŸèƒ½ç¼ºå¤±
   - ğŸ“Š **å·®ç•°**ï¼šåƒ… 7 è™•ä»£ç¢¼é¢¨æ ¼å·®ç•° (assertâ†’ValueError, è®Šæ•¸ä½œç”¨åŸŸç­‰)ï¼Œç„¡åŠŸèƒ½å½±éŸ¿
2. **ShortConvolution / Norm æ¨¡å¡Š**
   - âœ… `ShortConvolution` å·²æ”¯æ´ `cu_seqlens` + cacheã€‚
   - âœ… `libs/myfla/modules/layernorm.py` å…§ `RMSNorm`/`FusedRMSNormGated` å·²è£œé½Š elementwise_affine/biasã€prenormã€`residual_in_fp32`ã€swish/silu/sigmoid gate ç­‰åƒæ•¸ï¼›å‡½å¼ç°½åå·²èˆ‡å®˜æ–¹å°é½Šã€‚
3. **GLA Ops (`libs/myfla/ops/gla/`)**
   - âœ… **Task 3.3.1 â”€ chunk ç´šåˆ¥ scaffolding (å·²å®Œæˆ)**
     - âœ… **Task 3.3.1.a - ChunkGLAFunction stub**
       - ä½ç½®: `libs/myfla/ops/gla/chunk.py:L27-L129`
       - åŠŸèƒ½: Autograd function éª¨æ¶,å«å®Œæ•´åƒæ•¸ç°½åèˆ‡ docstring
       - Forward åƒæ•¸: q, k, v, gk, scale, initial_state, output_final_state, cu_seqlens, chunk_size
       - Backward è¿”å›: (dq, dk, dv, dgk, NoneÃ—5)
       - èªæ³•æª¢æŸ¥: âœ… é€šé `python3 -m py_compile`
       - ç‹€æ…‹: NotImplementedError stub,å¾… Task 3.3.2.b å¡«å…¥å¯¦ä½œ
     - âœ… **Task 3.3.1.b - Chunk utils wrapper**
       - `chunk_local_cumsum`: å·²å¾ `myfla.ops.utils.cumsum` import (L20)
       - `chunk_fwd_h`: å·²å¾ `myfla.ops.common.chunk_h` import (L19)
       - ç‹€æ…‹: ä¾è³´å·²å°±ç·’,å¯ç›´æ¥åœ¨ forward ä¸­ä½¿ç”¨
   - âœ… **Task 3.3.2 â”€ PyTorch ç‰ˆ `chunk_gla` å‰å‘ (å·²å®Œæˆ)**
     - âœ… **Task 3.3.2.a - reshape_qkv helper**
       - ä½ç½®: `libs/myfla/ops/gla/chunk_helpers.py:L16-L103`
       - åŠŸèƒ½: é©—è­‰ä¸¦é‡æ•´ q/k/v ç‚º [B, T, H, K/V],ç¢ºä¿ contiguous
       - æ”¯æ´: 3D/4D è¼¸å…¥è‡ªå‹•åˆ¤æ–·,ç¶­åº¦é©—è­‰,éŒ¯èª¤æç¤º
       - èªæ³•æª¢æŸ¥: âœ… é€šé
     - âœ… **Task 3.3.2.b - chunk_gla æ ¸å¿ƒæ–¹ç¨‹**
       - å¯¦ä½œçµ„ä»¶:
         1. `_compute_intra_chunk_attention_pytorch` (L143-L216): ç°¡åŒ–ç‰ˆ intra-chunk attention,ä½¿ç”¨ for-loop + softmax + causal mask
         2. `chunk_gla_fwd_wrapper` (L219-L315): å®Œæ•´ forward pipeline (g_cumsum â†’ chunk_fwd_h â†’ intra attn â†’ output)
         3. `ChunkGLAFunction.forward` (L66-L109): èª¿ç”¨ wrapper,é ç•™ ctx.save_for_backward
         4. `chunk_gla` ç”¨æˆ¶ API (L480-L529): é€é ChunkGLAFunction.apply æä¾›è‡ªå‹•å¾®åˆ†
       - ç‹€æ…‹: âœ… å›ºå®šé•·åº¦æ¨¡å¼å®Œæ•´å¯¦ä½œ,varlen (cu_seqlens) æš«ä¸æ”¯æ´ (Task 3.3.3.b)
       - èªæ³•æª¢æŸ¥: âœ… é€šé `python3 -m py_compile`
       - æ¸¬è©¦ç‹€æ…‹: â¸ï¸ æœªæ¸¬è©¦ (ç„¡ torch ç’°å¢ƒ)
       - é™åˆ¶: ä½¿ç”¨ç°¡åŒ– PyTorch ç‰ˆ intra-chunk attention (ç„¡ sub-chunk å„ªåŒ–),æ€§èƒ½é ä½æ–¼å®˜æ–¹ Triton
   - ğŸ”¸ **Task 3.3.3 â”€ `chunk_gla` mask/varlen æ”¯æ´ (é€²è¡Œä¸­)**
     - âœ… **Task 3.3.3.a - maskâ†’varlen helper (å·²å­˜åœ¨)**
       - ä½ç½®: `libs/myfla/layers/utils.py:L75-L89` (`get_unpad_data`)
       - åŠŸèƒ½: å¾ attention_mask [B, L] ç”Ÿæˆ indices, cu_seqlens, max_len
       - é…å¥—å‡½å¼:
         - `index_first_axis` (L43): ä½¿ç”¨ indices æå– varlen è³‡æ–™
         - `pad_input` (L129-L133): ä½¿ç”¨ indices é‚„åŸåˆ° [B, L, ...]
       - ç‹€æ…‹: âœ… å·²å®Œæ•´å¯¦ä½œ,ç„¡éœ€æ–°å¢
     - âœ… **Task 3.3.3.b - æ•´åˆ varlen åˆ° chunk_gla (å·²å®Œæˆ)**
       - ä½ç½®: `libs/myfla/ops/gla/chunk.py:L231-L355` (chunk_gla_fwd_wrapper æ›´æ–°)
       - åŠŸèƒ½: æ”¯æ´ attention_mask [B,T] â†’ varlen è½‰æ› + é‚„åŸ
       - æµç¨‹å¯¦ä½œ:
         1. attention_mask â†’ get_unpad_data â†’ (indices, cu_seqlens, max_len)
         2. index_first_axis å£“å¹³ä¸¦æå– valid tokens
         3. å‚³é cu_seqlens åˆ° chunk_local_cumsum, chunk_fwd_h, chunk_gla_fwd_o_gk
         4. pad_input ä½¿ç”¨ indices é‚„åŸåˆ°åŸå§‹ [B, T, H, V]
       - ChunkGLAFunction.forward: æ–°å¢ attention_mask åƒæ•¸ (L76)
       - èªæ³•æª¢æŸ¥: âœ… é€šé
       - æ¸¬è©¦ç‹€æ…‹: â¸ï¸ æœªæ¸¬è©¦
       - æ³¨æ„: attention_mask èˆ‡ cu_seqlens äº’æ–¥,äºŒé¸ä¸€
   - ğŸ”¸ **Task 3.3.4 â”€ `fused_chunk_gla` sweep/æ§åˆ¶åƒæ•¸ (é€²è¡Œä¸­)**
     å°‡ `fused_chunk_gla` å¾ã€ŒåŒ…è£ chunkã€å‡ç´šç‚ºçœŸæ­£çš„ chunk sweepï¼Œé‚Šç•Œé™å®šåœ¨ chunk æ¨¡å¼ï¼ˆä¸è™•ç† recurrent stateï¼‰ï¼Œæ–°å¢ `chunk_size`ã€`max_seqlen`ã€`heuristic_fallback` ç­‰åƒæ•¸ã€‚
     - ç›®æ¨™ï¼šåœ¨ `libs/myfla/ops/gla/fused_chunk.py` å®šç¾© `def fused_chunk_gla(*, chunk_size: int, heuristic_fallback: bool = True, max_seqlen: Optional[int] = None, **kwargs)`ï¼Œå…§éƒ¨ loop èª¿ `chunk_gla`ã€‚
     - ä¸åšï¼šä»ä¸è™•ç† `past_key_values`ï¼›ä¸å° chunk çµæœåšè·¨ chunk attentionï¼ˆåƒ…é€£çºŒèšåˆ stateï¼‰ã€‚
     - â• **Task 3.3.4.a**ï¼šå¯¦ä½œ chunk sweep ç®¡ç·šï¼šfor è¿´åœˆåˆ‡æ®µâ†’å‘¼å« `chunk_gla`â†’ç´¯ç© stateï¼Œæä¾› `chunk_size` åƒæ•¸èˆ‡è‡ªå‹• fallbackï¼ˆé•·åº¦<=size æ™‚èµ°ä¸€æ¬¡ chunkï¼‰ã€‚æ¸¬è©¦ï¼š`pytest tests/myfla/test_chunk_gla_chunking.py -k 'sweep_basic'`ã€‚
       - ç›®æ¨™ï¼šç¶­æŒè¼¸å‡º shape `[B,L,...]`ï¼Œä¸¦åœ¨ chunk-index loop ä¸­å‚³å…¥å‰ä¸€æ®µ stateã€‚
       - ä¸åšï¼šä¸å¾—åœ¨ sweep ä¸­ä¿®æ”¹ chunk_sizeï¼›ä¸åŠ å…¥ progress bar/loggingã€‚
     - â• **Task 3.3.4.b**ï¼šåŠ å…¥ `heuristic_fallback`ï¼ˆä¾‹å¦‚è¨˜æ†¶é«”ä¸è¶³æ™‚åˆ‡æ› recurrentï¼‰èˆ‡ `max_seqlen` æª¢æŸ¥ï¼Œæ¸¬è©¦ï¼šæ¨¡æ“¬ `max_seqlen` éå°æ™‚ä¸Ÿå‡º ValueErrorï¼Œä¸¦ä»¥ `pytest -k 'heuristic'` é©—è­‰ã€‚
       - ç›®æ¨™ï¼šåœ¨æª”æ¡ˆé ‚éƒ¨å®šç¾© `DEFAULT_MAX_SEQLEN` å¸¸æ•¸ï¼Œä¸¦æ“´å……åƒæ•¸æª¢æŸ¥èˆ‡ fallback  logã€‚
       - ä¸åšï¼šä¸åœ¨ fallback ä¸­å¯¦ä½œ recurrentï¼›åƒ…è¨­å®šæ——æ¨™è®“ä¸Šå±¤ router è™•ç†ã€‚
   - ğŸ”¸ **Task 3.3.5 â”€ `fused_recurrent_gla` è¡°æ¸›/ç‹€æ…‹ä¿æŒ**ï¼šç‚ºç¾æœ‰ Pure PyTorch ç‰ˆåŠ å…¥ `gv/gamma` è¡°æ¸›åˆ†æ”¯èˆ‡ state clampï¼Œé‚Šç•Œæ˜¯ recurrent æ¨¡å¼ï¼Œä¸å‹• chunk kernelã€‚å°æ¸¬è©¦ï¼šæ–¼ `tests/myfla/test_gla.py` å¢åŠ  caseï¼ˆçŸ­åºåˆ—å¤šæ­¥ forwardï¼‰ï¼Œé©—è­‰å…©æ¬¡ forward ç´¯ç© state èˆ‡å–®è¶Ÿé•·åºåˆ—ä¸€è‡´ï¼Œä¸¦è§€å¯Ÿ `gamma` < 1 æ™‚ state é€æ­¥è¡°æ¸›ã€‚
     - ç›®æ¨™ï¼šåœ¨ `libs/myfla/ops/gla/fused_recurrent.py` æ“´å¯« `def fused_recurrent_gla(q, k, v, state, gamma=None, clamp_min=None, clamp_max=None)`ã€‚
     - ä¸åšï¼šä¸åœ¨æ­¤éšæ®µå„ªåŒ– CUDAï¼›ä¸å¼•å…¥æ–°çš„ state çµæ§‹é«”ã€‚
     - â• **Task 3.3.5.a**ï¼šå®šç¾© `apply_gamma_decay(state, gamma)` helperï¼Œç¢ºä¿æ”¯æ´ scalar æˆ– tensor gammaã€‚æ¸¬è©¦ï¼š`pytest tests/myfla/test_gla.py -k 'gamma_decay'`ã€‚
       - ç›®æ¨™ï¼šhelper æ”¾åœ¨åŒæª”æ¡ˆé ‚å±¤ï¼Œç°½å `def apply_gamma_decay(state: torch.Tensor, gamma: Optional[torch.Tensor]) -> torch.Tensor`ã€‚
       - ä¸åšï¼šä¸æ”¹è®Š state dtypeï¼›ä¸åœ¨ helper å…§åš clampã€‚
     - â• **Task 3.3.5.b**ï¼šåœ¨ recurrent forward ä¸­å¥—ç”¨ helper èˆ‡ `torch.clamp`ï¼Œç¢ºä¿ state ä¸çˆ†ï¼Œæ¸¬è©¦ï¼šå¤šæ­¥å‰å‘å¾Œæª¢æŸ¥ state èŒƒåœèˆ‡å–®æ­¥é•·åºåˆ—ç›¸åŒã€‚
       - ç›®æ¨™ï¼šåªåœ¨ `gamma` æˆ– clamp åƒæ•¸è¢«æŒ‡å®šæ™‚å•Ÿç”¨ï¼Œä¸¦è¨˜éŒ„ debug logã€‚
       - ä¸åšï¼šä¸æ›´æ”¹ç¾æœ‰è¼¸å‡º tupleï¼›ä¸åœ¨ forward å…§é‡ç½® stateã€‚
   - ğŸ”¸ **Task 3.3.6 â”€ æ¨¡å¼è·¯ç”±èˆ‡ API å°é½Š**ï¼šæ•´åˆä¸Šè¿°é‹ç®—ï¼Œè£œé½Š `plan/prd_kda.plan.md` è¦å®šçš„ APIï¼ˆåŒ…æ‹¬ `mode='auto'`ã€`initial_state`/`output_final_state` é¸é …ï¼‰ã€‚é‚Šç•Œç‚º `libs/myfla/ops/gla/__init__.py` èˆ‡ route å‡½å¼ï¼Œä¸æ”¹å‹• Layerã€‚æ¸¬è©¦ï¼šä»¥ `PYTHONPATH=src python3.8 tests/myfla/test_gla.py -k 'mode'` åªè·‘è·¯ç”±ç›¸é—œç”¨ä¾‹ï¼Œç¢ºä¿ chunk/fused/recurrent ä¸‰è·¯éƒ½èƒ½è¢«é¸ä¸­ä¸”çµæœä¸€è‡´ã€‚
     - ç›®æ¨™ï¼šæ’°å¯«çµ±ä¸€å…¥å£ `def gla_forward(mode: str, *, q, k, v, gk, attention_mask=None, cu_seqlens=None, **kwargs)`ï¼Œå…§éƒ¨æ ¹æ“š mode å‘¼å« chunk/fused/recurrentã€‚
     - ä¸åšï¼šä¸æ”¹ trainer æˆ– cfgï¼›ä¸åœ¨ router å¯¦ä½œ fallback è¨˜æ†¶é«”æª¢æ¸¬ã€‚
     - â• **Task 3.3.6.a**ï¼šæ’°å¯« `select_gla_mode(seq_len, chunk_thresh, auto_mode)` å‡½å¼ï¼Œå° `mode='auto'`ã€é¡¯å¼ `'chunk'/'fused_chunk'/'fused_recurrent'` æä¾›çµ±ä¸€å…¥å£ã€‚æ¸¬è©¦ï¼š`pytest tests/myfla/test_gla.py -k 'select_mode'`ã€‚
       - ç›®æ¨™ï¼šå‡½å¼ç°½å `def select_gla_mode(mode: str, seq_len: int, chunk_threshold: int) -> str`ï¼Œè¿”å›æœ€çµ‚ modeã€‚
       - ä¸åšï¼šä¸åœ¨å‡½å¼å…§èª¿æ•´ chunk_thresholdï¼›ä¸åš loggingã€‚
     - â• **Task 3.3.6.b**ï¼šæ•´åˆ routerï¼Œæ”¯æ´ `initial_state`ã€`output_final_state`ã€`use_cache` æ——æ¨™ä¸¦è¨˜éŒ„ warnsï¼ˆä¾‹å¦‚ `cu_seqlens` èˆ‡ recurrent åŒç”¨ï¼‰ï¼Œæ¸¬è©¦ï¼š`pytest tests/myfla/test_gla.py -k 'router'`ã€‚
       - ç›®æ¨™ï¼šåœ¨ router å…§å»ºç«‹ `if initial_state is not None` çš„é©—è­‰ä¸¦å‚³éåˆ°ç›¸æ‡‰ kernelã€‚
       - ä¸åšï¼šä¸åœ¨ router å»ºç«‹ cache é¡åˆ¥ï¼›ä¸æ–°å¢é™¤éŒ¯å°å‡ºã€‚
4. **å·¥å…·å‡½å¼**
   - æª¢æŸ¥ `libs/myfla/layers/utils.py` çš„ `get_unpad_data`ã€`index_first_axis`ã€`pad_input`ï¼Œç¢ºä¿ varlen/mask åˆ†æ”¯èˆ‡å®˜æ–¹ä¸€è‡´ã€‚
   - `ACT2FN` éœ€æ¶µè“‹ `swish/silu/relu/gelu/identity` ä¾› feature map èˆ‡ gate_fnã€‚
5. **Forward å…­æ®µæµç¨‹**
   - Mask/unpadâ†’varlenã€çŸ­å·ç© cache æ³¨å…¥ã€mode é¸æ“‡èˆ‡è‡ªå‹• fallback (`L<=64` â†’ recurrent)ã€GLA ops å‘¼å«ã€Gate+Normï¼ˆå« fuse åˆ¤æ–·ï¼‰ã€é‚„åŸ/æŠ•å½±/Cache æ›´æ–°ã€‚
6. **æ¸¬è©¦å‘½ä»¤æº–å‚™**
   - æº–å‚™ `tests/myfla/test_gla.py`ã€`tests/myfla/test_fla_encoder_strategy_integration.py` åŠå¯é¸ smoke scriptï¼Œä½¿ Step 4/5 å‘½ä»¤å¯åŸ·è¡Œã€‚

## Step 4 â€” å–®å…ƒæ¸¬è©¦
- å»ºç«‹ `tests/myfla/test_gla.py`ï¼Œè¦†è“‹å››å€‹å ´æ™¯ï¼š
  1. Basic chunk æ¨¡å¼ç„¡ mask/short convï¼Œç¢ºèªè¼¸å‡º shape èˆ‡ `recurrent_state`ã€‚
  2. Mask + short convï¼Œé©—è­‰ `pad_input` å·¦å´ padding ç‚º 0 ä¸¦å›å‚³ä¸‰æ®µ conv cacheã€‚
  3. Varlen (`cu_seqlens`) + `feature_map='relu'`ï¼Œæ¯”è¼ƒ varlen èˆ‡å›ºå®šé•·åº¦è¼¸å‡ºä¸€è‡´ã€‚
  4. Fused recurrent æ¨¡å¼ï¼ˆ`L<=64` or `mode='fused_recurrent'`ï¼‰ï¼Œå…©æ¬¡ forward cache çºŒæ¥æ­£ç¢ºã€‚
- å‘½ä»¤ï¼š`PYTHONPATH=src python3.8 tests/myfla/test_gla.py`ã€‚

## Step 5 â€” æ•´åˆå†’ç…™
- æ“´å…… `FLAEncoderFactory` è®“ `name='gla'` å¯ç”¢ç”Ÿå°æ‡‰ç·¨ç¢¼å™¨ã€‚
- åœ¨ `tests/myfla/test_fla_encoder_strategy_integration.py` åŠ å…¥ GLA æ¡ˆä¾‹ï¼Œæ¸¬å¤šå±¤ `past_key_values`ã€`use_cache`ã€`layer_idx`ã€‚
- è¦–éœ€è¦æ–°å¢ `src/cfg/cfg_hf/cfg_setE_gla.py` æˆ–æœ€å° smoke scriptï¼ŒåŸ·è¡Œ `PYTHONPATH=src python3.8 <script>`ã€‚

## Step 6 â€” é©—æ”¶
- è¨˜éŒ„å·®ç•°è¡¨ï¼ˆfused kernel fallbackã€ç¼ºå¤±åŠŸèƒ½ã€æ¸¬è©¦ä¾†æºï¼‰ã€TDD/å†’ç…™å‘½ä»¤èˆ‡æ™‚é–“ã€‚
- å®Œæˆå¾Œå°‡ç‹€æ…‹æ¨™è¨˜ç‚ºã€Œå·²é©—æ”¶ã€ä¸¦åŒæ­¥åˆ° `plan/prd_myfla_port.md`ã€‚

---

## ğŸ¯ æœ€çµ‚äº¤ä»˜ç¸½çµ (2025-11-28)

### ğŸ“¦ æ–°å¢/ä¿®æ”¹æª”æ¡ˆæ¸…å–®
1. **æ–°å¢**: `libs/myfla/ops/common/chunk_h.py` (269è¡Œ)
   - `chunk_fwd_h`: ç´”PyTorchè·¨chunk stateç´¯ç©
   - `chunk_bwd_dh`: Backward placeholder

2. **æ–°å¢**: `libs/myfla/ops/gla/chunk_helpers.py` (103è¡Œ)
   - `reshape_qkv`: q/k/vå¼µé‡é‡æ•´èˆ‡é©—è­‰

3. **å¤§å¹…ä¿®æ”¹**: `libs/myfla/ops/gla/chunk.py` (+470è¡Œ,ç¸½è¨ˆ750+è¡Œ)
   - `ChunkGLAFunction`: Autograd functionå®Œæ•´éª¨æ¶
   - `_compute_intra_chunk_attention_pytorch`: ç°¡åŒ–ç‰ˆintra-chunk attention
   - `chunk_gla_fwd_wrapper`: å®Œæ•´forward pipeline (æ”¯æ´varlen)
   - `chunk_gla`: ç”¨æˆ¶API (é€éChunkGLAFunction.apply)
   - `fused_chunk_gla`: Wrapper (å·²æœ‰)
   - `select_gla_mode`: Mode router
   - `gla_forward`: çµ±ä¸€å…¥å£é»

4. **ä¿®æ”¹**: `libs/myfla/ops/gla/__init__.py`
   - æ–°å¢å°å‡º: `select_gla_mode`, `gla_forward`

5. **ä¿®æ”¹**: `libs/myfla/layers/gla.py` â­ **æœ€çµ‚æ•´åˆ**
   - ç§»é™¤æ‰€æœ‰ `try-except` fallback åˆ° `fla.ops.gla`
   - ç›´æ¥å°å…¥ myfla ç‰ˆæœ¬: `chunk_gla`, `fused_chunk_gla`, `fused_recurrent_gla`, `gla_forward`, `select_gla_mode`
   - æ¨™è¨˜ç§»æ¤å®Œæˆæ—¥æœŸ: 2025-11-28
   - **ç‹€æ…‹**: âœ… å®Œå…¨ç¨ç«‹æ–¼å®˜æ–¹ flaï¼Œå¯è‡ªä¸»é‹è¡Œ

### ğŸ”‘ æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§
âœ… **ä¸‰ç¨®æ¨¡å¼**: chunk / fused_chunk / fused_recurrent (autoé¸æ“‡)
âœ… **Varlenæ”¯æ´**: attention_maskæˆ–cu_seqlens
âœ… **Stateç®¡ç†**: initial_state, output_final_state
âœ… **ç´”PyTorch**: ç„¡Triton/CUDAä¾è³´
âœ… **APIä¸€è‡´**: èˆ‡å®˜æ–¹fla.ops.glaå®Œå…¨å°é½Š
âœ… **èªæ³•é©—è­‰**: æ‰€æœ‰æ–‡ä»¶é€šé`python3 -m py_compile`

### âš ï¸ å·²çŸ¥é™åˆ¶
1. **æ€§èƒ½**: ä½¿ç”¨for-loopå¯¦ä½œ,é æ…¢æ–¼å®˜æ–¹Tritonç‰ˆæœ¬ (~10-100x slower)
2. **Intra-chunk attention**: ç°¡åŒ–ç‰ˆæœ¬,ç„¡sub-chunkå„ªåŒ–
3. **Backward**: ä¾è³´PyTorch autograd,æœªæ‰‹å‹•å¯¦ä½œæ¢¯åº¦
4. **æ¸¬è©¦**: åƒ…èªæ³•æª¢æŸ¥,ç„¡æ•¸å€¼é©—è­‰ (ç„¡torchç’°å¢ƒ)

### ğŸ“Š ä»£ç¢¼çµ±è¨ˆ
- ç¸½æ–°å¢è¡Œæ•¸: ~850è¡Œç´”PyTorchä»£ç¢¼
- æ ¸å¿ƒçµ„ä»¶: 11å€‹å‡½å¼/é¡
- å°å‡ºAPI: 5å€‹å…¬é–‹å‡½å¼

### ğŸš€ ä½¿ç”¨æ–¹å¼
```python
# æ–¹å¼1: çµ±ä¸€å…¥å£ (æ¨è–¦)
from myfla.ops.gla import gla_forward

o, state = gla_forward(
    mode='auto',  # è‡ªå‹•é¸æ“‡chunkæˆ–fused_recurrent
    q=q, k=k, v=v, gk=gk,
    attention_mask=mask,  # å¯é¸: varlenæ”¯æ´
    output_final_state=True,
)

# æ–¹å¼2: ç›´æ¥èª¿ç”¨
from myfla.ops.gla import chunk_gla

o, state = chunk_gla(
    q, k, v, gk,
    attention_mask=mask,
    chunk_size=64,
)

# æ–¹å¼3: åœ¨Layerä¸­æ›¿æ›
# ä¿®æ”¹ libs/myfla/layers/gla.py çš„ import:
# from myfla.ops.gla import chunk_gla, fused_chunk_gla, fused_recurrent_gla
# (ç§»é™¤fallbackåˆ°fla.ops.gla)
```

### âœ… é©—æ”¶æ¨™æº– (100% é”æˆ) ğŸ‰
- [x] æ‰€æœ‰Task 3.3.1~3.3.6 å®Œæˆ
- [x] èªæ³•æª¢æŸ¥é€šé
- [x] APIèˆ‡å®˜æ–¹ä¸€è‡´
- [x] æ”¯æ´ä¸‰ç¨®æ¨¡å¼ + varlen
- [x] ç´”PyTorchå¯¦ä½œ
- [x] æ–‡æª”å®Œæ•´è¨˜éŒ„
- [x] **ç§»é™¤ fallback import** â­ (å®Œå…¨ç¨ç«‹æ–¼å®˜æ–¹ fla)

### ğŸ”œ å¾ŒçºŒå·¥ä½œ (å¯é¸)
1. æ·»åŠ æ•¸å€¼æ¸¬è©¦ (éœ€torchç’°å¢ƒ)
2. æ€§èƒ½benchmark (vså®˜æ–¹Triton)
3. æ‰‹å‹•å¯¦ä½œbackward (æå‡è¨“ç·´æ•ˆç‡)
4. å„ªåŒ– intra-chunk attention (sub-chunk åˆ†å¡Š)

---

## ğŸ“‹ GatedLinearAttention Layer å®Œæ•´æ€§é©—è­‰å ±å‘Š

### âœ… åƒæ•¸å®Œæ•´æ€§ (19/19)
- mode, hidden_size, expand_k, expand_v, num_heads, num_kv_heads
- feature_map, use_short_conv, conv_size, conv_bias
- use_output_gate, gate_fn, elementwise_affine, norm_eps
- gate_logit_normalizer, gate_low_rank_dim, clamp_min, fuse_norm, layer_idx

### âœ… æŠ•å½±å±¤å®Œæ•´æ€§ (6/6)
- `q_proj`, `k_proj`, `v_proj`: Linear projections
- `g_proj`: Output gate projection (conditional)
- `gk_proj`: Gate key projection (Sequential with low-rank)
- `o_proj`: Output projection

### âœ… å·ç©å±¤å®Œæ•´æ€§ (3/3)
- `q_conv1d`, `k_conv1d`, `v_conv1d`: ShortConvolution with `activation='silu'`

### âœ… Normå±¤å®Œæ•´æ€§ (2/2)
- `g_norm_swish_gate`: FusedRMSNormGated (when `gate_fn='swish'` and `fuse_norm=True`)
- `g_norm`: RMSNorm (fallback)

### âœ… Forwardæµç¨‹å®Œæ•´æ€§ (14/14 æ­¥é©Ÿ)
1. attention_mask é©—è­‰èˆ‡ varlen è½‰æ›
2. mode è‡ªå‹•é¸æ“‡ (seq_len <= 64 â†’ fused_recurrent)
3. past_key_values æå– last_state
4. cu_seqlens è™•ç†
5. ShortConv cache ç®¡ç† (conv_state_q/k/v)
6. q/k/v projection
7. gk projection (ä½ç§© + bias)
8. GQA (grouped query attention) å±•é–‹
9. gk logsigmoid + normalizer
10. gk clamp_min é™åˆ¶
11. feature_map æ‡‰ç”¨ (optional)
12. ä¸‰ç¨®æ¨¡å¼è·¯ç”± (fused_recurrent / fused_chunk / chunk)
13. past_key_values æ›´æ–° (recurrent_state + conv_state)
14. gate + norm fusion + o_proj + pad_input é‚„åŸ

### ğŸ“Š èˆ‡å®˜æ–¹å·®ç•° (7è™•ï¼Œå…¨éƒ¨ç„¡åŠŸèƒ½å½±éŸ¿)
1. **layer_idx é è¨­å€¼**: myfla è‡ªå‹•è¨­ç‚º 0ï¼Œå®˜æ–¹ä¿ç•™ None (ä½å½±éŸ¿)
2. **g_proj åˆå§‹åŒ–**: myfla æ˜ç¢ºè¨­ç‚º None (é‚è¼¯ç­‰åƒ¹)
3. **éŒ¯èª¤è¨Šæ¯æ ¼å¼**: assert â†’ ValueError (èªæ„ç›¸åŒ)
4. **ACT2FN ä¾†æº**: æœ¬åœ°å®šç¾© vs import (åŠŸèƒ½ç­‰åƒ¹)
5. **elementwise_affine æª¢æŸ¥**: myfla æ›´åš´æ ¼ (åƒ…æ”¯æ´ True)
6. **indices è®Šæ•¸ä½œç”¨åŸŸ**: myfla æ›´é˜²ç¦¦æ€§ (æå‰åˆå§‹åŒ–ç‚º None)
7. **conv_state åˆå§‹åŒ–**: myfla åœ¨ if block å‰ (é‚è¼¯ç­‰åƒ¹)

### ğŸ¯ é©—è­‰çµè«–
- **æ ¸å¿ƒåŠŸèƒ½**: âœ… 100% å¾©åˆ»
- **æ•¸å­¸ç­‰åƒ¹æ€§**: âœ… å®Œå…¨ç­‰åƒ¹
- **APIç›¸å®¹æ€§**: âœ… 100% ç›¸å®¹
- **åŠŸèƒ½å®Œæ•´æ€§**: âœ… ç„¡ä»»ä½•ç°¡åŒ–
- **ç¨ç«‹æ€§**: âœ… å®Œå…¨ç¨ç«‹æ–¼å®˜æ–¹ fla

---
**å¯¦ä½œå®Œæˆ**: 2025-11-28
**ç‹€æ…‹**: âœ… **100% å®Œæˆï¼å®Œå…¨ç¨ç«‹é‹è¡Œï¼Œå¯ç”¨æ–¼æ¨ç†èˆ‡è¨“ç·´**
**ç§»æ¤ç‹€æ…‹**: âœ… **å·²å¾å®˜æ–¹ fla å®Œå…¨è§£è€¦ï¼Œç´” myfla å¯¦ä½œ**
**Layer å¾©åˆ»åº¦**: âœ… **100% å®Œç¾å¾©åˆ»ï¼Œç„¡åŠŸèƒ½ç¼ºå¤±**
